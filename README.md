# AutoEmbed

AutoEmbed is an ML framework for Retrieval-Augmented Generation (RAG) that automatically selects the Pareto optimal chunking strateg, size, and embedding model for any document corpus to maximize retrieval performance under cost and latency constraints.

---

## ğŸš€ Features

- **Multi-format Ingestion**: Load and normalize text from PDF, Markdown, and HTML documents.
- **Pluggable Chunking Engine**:  
  - Sentence-Aware (semantic boundaries)  
  - Fixed-Token (uniform N-token chunks)  
  - Sliding-Window (overlapping token windows)  
- **Embedding Interface**:  
  - OpenAIEmbedder (OpenAI API)  
  - HFEmbedder (Hugging Face Transformers)
- **Vector Store Integration**:  
  - Qdrant via Docker Compose  
  - Upsert & search with metadata filters
- **Automated Evaluation Harness**:  
  - LLM-generated or manually authored gold questions  
  - Standard IR metrics: Recall@K, MRR, Precision@K, F1 overlap, Exact Match  
- **Optimization Loop**: Grid/random search over strategies, models, chunk sizes, and overlap â†’ Pareto frontier & recommended config
- **Interactive CLI & Demo**:  
  - `autoembed ingest|chunk|embed|search|eval|optimize` commands  
  - (Optional) Streamlit UI for upload and one-click evaluation

---

## ğŸ“ Repository Structure

```
AutoEmbed/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml      # Pipeline parameters & objectives
â”œâ”€â”€ data/                 # Example raw files (PDF, MD, HTML)
â”œâ”€â”€ ingested/             # Normalized JSON outputs from ingestion
â”œâ”€â”€ chunks/               # Chunk JSONs per strategy
â”œâ”€â”€ embeddings/           # (Optional) Cached vectors
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py         # YAML loader
â”‚   â”œâ”€â”€ ingestion.py      # File readers
â”‚   â”œâ”€â”€ chunk_router.py   # Strategy dispatcher
â”‚   â”œâ”€â”€ run_chunking.py   # CLI to generate chunks
â”‚   â”œâ”€â”€ embeddings.py     # Embedder interface & factory
â”‚   â”œâ”€â”€ run_embedding.py  # Embed & upsert to Qdrant
â”‚   â”œâ”€â”€ vectorstore.py    # Qdrant wrapper (upsert/search)
â”‚   â”œâ”€â”€ run_search.py     # Interactive queryâ†’results
â”‚   â”œâ”€â”€ eval.py           # Batch eval & metrics
â”‚   â””â”€â”€ optimize.py       # ConfigTester & Pareto loop
â”œâ”€â”€ docker-compose.yml    # Qdrant service
â”œâ”€â”€ generate_gold_qa.py   # LLM-based QA generation
â”œâ”€â”€ gold_queries.yaml     # Example gold query file
â”œâ”€â”€ logs/                 # Evaluation & cost logs
â””â”€â”€ README.md             # This file
```

---

## âš™ï¸ Setup Instructions

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-org/AutoEmbed.git
   cd AutoEmbed
   ```

2. **Install dependencies**
   The key libraries are:
   - `openai`, `transformers`, `torch`
   - `qdrant-client`
   - `PyYAML` for config

3. **Configure API keys in a json APIKeys.json at the root**
   ```bash
   export OPENAI_API_KEY="sk-..."
   ```

4. **Start Qdrant**
   ```bash
   docker compose up -d
   ```
  
5. **Put your own corpus (files) in data/**

5. **Edit `config/default.yaml`** to set your chunking, embedding, and evaluation objectives.

---

## ğŸƒ Quick Start

   ```bash
  python3 src/run_ragged.py
   ```
   runs everything

---

## ğŸ“˜ License & Contribution

MIT License. Contributions welcome via pull requests or issues on GitHub.

