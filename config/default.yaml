#A sample configuration file:
#Which filetypes to ingest (pdf, markdown, html)
#Chunking strategy and its parameters (window size, overlap, etc.)
#Which metadata fields to extract


# ===============================
# üåê AutoEmbed: Default Config
# ===============================

# General Ingestion Settings
ingestion:
  input_folder: "../data/"
  accepted_filetypes: ["pdf", "markdown", "html"]
  include_metadata: true

# ===============================
# üß© Chunking Configuration
# ===============================
strats:
  - "fixed_token"
  - "sliding_window"
  - "sentence_aware"
 

  

sentence_max_tokens: 300

fixed_chunk_size: 10 

overlap: 5

# ===============================
# üî§ Tokenizer / Embedding Model
# ===============================
embedding:
  - provider: "openai"
    model: "text-embedding-3-small"
  - provider: "huggingface"
    model: "sentence-transformers/all-MiniLM-L6-v2"

# Used if provider == openai
openai:
  - model: "text-embedding-3-small"
    pricing_per_1k_tokens: 0.00002
  - model: "text-embedding-3-large"
    pricing_per_1k_tokens: 0.00013

# Used if provider == huggingface
huggingface:
  - model: "sentence-transformers/all-MiniLM-L6-v2"
  - model: "sentence-transformers/all-mpnet-base-v2"

# ===============================
# ‚öôÔ∏è Optimization Objectives
# ===============================
objectives:
  quality_target: "maximize"       # Options: maximize, min_recall@k, etc.
  max_latency_ms: 200              # Per-query latency constraint
  max_cost_per_10k_docs: 2.00      # Total embedding cost budget
  retrieval_top_k: 5               # Used in recall@k

# ===============================
# üìä Evaluation Settings
# ===============================
evaluation:
  generate_questions: true
  num_questions_per_doc: 3
  use_gpt4_for_answers: true       # Or manual, Claude, etc.
  eval_metrics: ["recall@k", "f1_overlap", "exact_match"]

# ===============================
# üß™ Experiment Metadata
# ===============================
experiment:
  name: "baseline_legal_docs_test"
  notes: "Testing chunk size vs. cost tradeoffs"
  save_outputs: true
  output_folder: "./logs/"

# Qdrant configuration
qdrant:
  url: "http://localhost:6333"
  api_key: ""  # Add your Qdrant API key here if needed

# User settings
user:
  default_collection_prefix: "autoembed_chunks_"
  vector_size: 1536  # Default vector size for OpenAI embeddings